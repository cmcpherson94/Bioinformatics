import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    precision_score, 
    recall_score, 
    f1_score, 
    roc_auc_score, 
    confusion_matrix
)
import networkx as nx

class PPIDatasetValidator:
    def __init__(self, datasets):
        """
        Initialize PPI Dataset Validator
        
        :param datasets: List of dataset paths or DataFrames
        """
        self.raw_datasets = datasets
        self.combined_dataset = None
        self.processed_dataset = None
        
    def load_datasets(self):
        """
        Load and combine multiple PPI datasets
        """
        dataframes = []
        
        for dataset in self.raw_datasets:
            # Support both file paths and DataFrames
            if isinstance(dataset, str):
                df = pd.read_csv(dataset, sep='\t')
            else:
                df = dataset
            
            # Standardize column names
            df.columns = [col.lower() for col in df.columns]
            
            # Ensure key columns exist
            required_columns = ['protein1', 'protein2', 'interaction']
            for col in required_columns:
                if col not in df.columns:
                    raise ValueError(f"Missing required column: {col}")
            
            dataframes.append(df)
        
        # Combine datasets
        self.combined_dataset = pd.concat(dataframes, ignore_index=True)
        
        return self
    
    def preprocess_dataset(self, confidence_threshold=0.7):
        """
        Clean and preprocess the combined dataset
        
        :param confidence_threshold: Minimum interaction confidence
        """
        # Remove duplicates
        self.combined_dataset.drop_duplicates(subset=['protein1', 'protein2'], inplace=True)
        
        # Filter by confidence if column exists
        if 'confidence' in self.combined_dataset.columns:
            self.combined_dataset = self.combined_dataset[
                self.combined_dataset['confidence'] >= confidence_threshold
            ]
        
        # Create binary interaction label
        self.combined_dataset['interaction_label'] = (
            self.combined_dataset['interaction'] > 0
        ).astype(int)
        
        # Feature engineering
        self.processed_dataset = self._engineer_features()
        
        return self
    
    def _engineer_features(self):
        """
        Generate features for interaction prediction
        """
        # Placeholder for feature engineering. In a real implementation, we'd extract more complex features
        features = self.combined_dataset.copy()
        
        # Example basic features
        features['protein_length_diff'] = np.abs(
            features['protein1_length'] - features['protein2_length']
        )
        
        # One-hot encode protein families if available
        # protein_families = pd.get_dummies(features['protein1_family'])
        
        return features
    
    def split_dataset(self, test_size=0.3, random_state=42):
        """
        Split dataset into training and testing sets
        
        :param test_size: Proportion of dataset to include in test split
        :param random_state: Random seed for reproducibility
        """
        X = self.processed_dataset.drop(['interaction_label'], axis=1)
        y = self.processed_dataset['interaction_label']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            test_size=test_size, 
            random_state=random_state,
            stratify=y
        )
        
        return X_train, X_test, y_train, y_test
    
    def train_random_forest(self, X_train, y_train):
        """
        Train Random Forest Classifier
        
        :param X_train: Training features
        :param y_train: Training labels
        """
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # Train Random Forest
        rf_classifier = RandomForestClassifier(
            n_estimators=100, 
            random_state=42, 
            class_weight='balanced'
        )
        
        rf_classifier.fit(X_train_scaled, y_train)
        
        return rf_classifier, scaler
    
    def evaluate_model(self, model, X_test, y_test, scaler):
        """
        Evaluate model performance
        
        :param model: Trained classifier
        :param X_test: Test features
        :param y_test: Test labels
        :param scaler: Feature scaler
        """
        # Scale test features
        X_test_scaled = scaler.transform(X_test)
        
        # Predictions
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        # Performance metrics
        metrics = {
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred),
            'roc_auc': roc_auc_score(y_test, y_pred_proba),
            'confusion_matrix': confusion_matrix(y_test, y_pred)
        }
        
        return metrics
    
    def cross_validation(self, X, y, cv=5):
        """
        Perform cross-validation
        
        :param X: Features
        :param y: Labels
        :param cv: Number of cross-validation folds
        """
        rf_classifier = RandomForestClassifier(
            n_estimators=100, 
            random_state=42, 
            class_weight='balanced'
        )
        
        # Perform cross-validation
        cv_scores = cross_val_score(
            rf_classifier, X, y, 
            cv=cv, 
            scoring='f1'
        )
        
        return {
            'mean_f1_score': np.mean(cv_scores),
            'std_f1_score': np.std(cv_scores)
        }

def main():
    # Example usage
    # Replace with actual dataset paths or DataFrames
    datasets = [
        'biogrid_dataset.csv', 
        'string_dataset.csv', 
        'intact_dataset.csv'
    ]
    
    validator = PPIDatasetValidator(datasets)
    
    # Workflow
    (validator.load_datasets()
               .preprocess_dataset()
    )
    
    X_train, X_test, y_train, y_test = validator.split_dataset()
    
    model, scaler = validator.train_random_forest(X_train, y_train)
    
    performance_metrics = validator.evaluate_model(model, X_test, y_test, scaler)
    
    print("Performance Metrics:", performance_metrics)

if __name__ == "__main__":
    main()
